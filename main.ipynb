{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3d6c55-431e-4c5b-bc9b-65422b8acb30",
   "metadata": {},
   "source": [
    "# 판교 AI Challenge\n",
    "> 참치김치찌개팀<br>\n",
    "> 팀장 손찬영, 팀원 김민정 김하림 이두현 차현수\n",
    "* 과제명 : [아동 및 교통약자 보호를 위한 어린이 도로보행 위험행동 분류 과제]\n",
    "* 과제 링크 : https://www.aiconnect.kr/main/competition/privateDetail/200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716901e-fbd4-4137-a9c8-47394572907b",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2b8db-a350-4118-9775-fd1cfb04ad12",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b2715d-e7ab-477a-952a-c00204f3f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "import warnings\n",
    "\n",
    "import easydict\n",
    "import pandas as pd\n",
    "import torchvideo.datasets as datasets\n",
    "import torchvideo.samplers as samplers\n",
    "import torchvideo.transforms as VT\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm.optim.nadam as nadam\n",
    "import torch\n",
    "import torchcontrib\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score\n",
    "from source.focalloss import FocalLoss\n",
    "from source.label_smooth import LabelSmoothSoftmaxCEV2\n",
    "from source.model import C3D_model, R2Plus1D_model, R3D_model\n",
    "from source.model.utils.vit import TimeSformer\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.swa_utils import SWALR\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchcontrib.optim import SWA\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f275c544-b678-435e-a06e-c477cd466032",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205f173-f1d1-4f42-bd81-3009d7905e9b",
   "metadata": {},
   "source": [
    "## Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41bc00ea-9722-4ae8-b952-0ec781a2f362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {\n",
    "        ############## Experiment ##############\n",
    "        \"experiment\": \"EXP1\",  # 매번 바꿔준다.\n",
    "        \"project_dir\": os.getcwd(),  # '/home/stephencha/Hub/ai-challenge'\n",
    "        \"train\": True,\n",
    "        \"inference\": True,\n",
    "        \"submit_path\": \"./submit\",\n",
    "        ############## Dataset ##############\n",
    "        \"dataset_path\": \"./dataset/train\",\n",
    "        \"test_dataset_path\": \"./dataset/test\",\n",
    "        \"label_path\": \"./dataset/train_data.csv\",\n",
    "        \"clip_length\": 5,  # slice\n",
    "        \"frame_step\": 1,\n",
    "        \"num_workers\": 8,\n",
    "        \"autoaugment\": True,\n",
    "        \"num_classes\": 9,\n",
    "        ############## Model ##############\n",
    "        \"model\": \"TimeSformer\",  # Options: C3D, R2Plus1D, R3D, TimeSformer, Efficientnet_LSTM\n",
    "        \"attention_type\": \"divided_space_time\",\n",
    "        \"img_size\": 224,\n",
    "        \"pretrained_model\": \"./pretrained/TimeSformer_divST_96x4_224_K600.pyth\",  # ./pretrained/c3d-pretrained.pth, ./pretrained/TimeSformer_divST_96x4_224_K600.pyth, 'efficientnet_b4'\n",
    "        ############## Fine-Tuning ##############\n",
    "        \"randomseed\": False,\n",
    "        \"epoches\": 50,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"loss_function\": \"cross_entropy\",\n",
    "        \"schedular\": \"cosineannealingwarmrestarts\", # cosineannealingwarmrestarts, step\n",
    "        \"batch_size\": 36,  # Depends on VRAM\n",
    "        ############## GPU ##############\n",
    "        \"multi_gpu\": True, \n",
    "        \"device\": \"cuda\",  # \"cpu\" for debugging\n",
    "    }\n",
    ")\n",
    "NAME_ELEMENTS = [args.model, time.strftime(\"%m%d_%H%M\", time.localtime(time.time()))]\n",
    "MODEL_NAME = \"_\".join(NAME_ELEMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647c3f2c-dfea-465c-ae10-291e42d040e0",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2647f-cbf4-486e-a4e8-f2b34fc46bdf",
   "metadata": {},
   "source": [
    "## Randomseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cf009c-314d-440f-b976-d034366f71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.randomseed:\n",
    "    torch.manual_seed(args.randomseed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(args.randomseed)\n",
    "    random.seed(args.randomseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b40161-7dc2-4e73-a587-60e650e93170",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c4669-fb33-4f2d-bfac-cbeee3b3c63f",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d2614ff-50f5-4820-93bf-aa781a8cf2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_li = {\n",
    "    0: \"driveway_walk\", # person\n",
    "    1: \"fall_down\", # person\n",
    "    2: \"fighting\", # person\n",
    "    3: \"jay_walk\", # person\n",
    "    4: \"normal\",\n",
    "    5: \"putup_umbrella\", # person + umbrella\n",
    "    6: \"ride_cycle\", # person + bicycle\n",
    "    7: \"ride_kick\", # person + kickboard\n",
    "    8: \"ride_moto\", # person + motorcycle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0572762-ae40-43ae-9544-dedcc6382007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make string of class to numbers.\n",
    "def string_to_num(row):\n",
    "    if row['class'] == cls_li[0]:\n",
    "        row['class'] = 0\n",
    "    elif row['class'] == cls_li[1]:\n",
    "        row['class'] = 1\n",
    "    elif row['class'] == cls_li[2]:\n",
    "        row['class'] = 2\n",
    "    elif row['class'] == cls_li[3]:\n",
    "        row['class'] = 3\n",
    "    elif row['class'] == cls_li[4]:\n",
    "        row['class'] = 4\n",
    "    elif row['class'] == cls_li[5]:\n",
    "        row['class'] = 5\n",
    "    elif row['class'] == cls_li[6]:\n",
    "        row['class'] = 6\n",
    "    elif row['class'] == cls_li[7]:\n",
    "        row['class'] = 7\n",
    "    elif row['class'] == cls_li[8]:\n",
    "        row['class'] = 8\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af02ef-fda0-4d96-a56e-84c2592b6f10",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63357c2-7974-4c66-885d-fb539bd1cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(args.label_path).set_index('video_filename')\n",
    "df = df.apply(string_to_num, axis='columns')\n",
    "df = df.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff548e12-440e-4c3e-b986-279d61270bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_filename</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>video_0000.mp4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_0001.mp4</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_0002.mp4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_0003.mp4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_0004.mp4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                class\n",
       "video_filename       \n",
       "video_0000.mp4      5\n",
       "video_0001.mp4      7\n",
       "video_0002.mp4      1\n",
       "video_0003.mp4      0\n",
       "video_0004.mp4      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336123c-1ec9-43bc-ae8b-eaa932776433",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694c0e72-2c3c-48d4-936e-710472440d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LABEL\n",
    "label = datasets.CsvLabelSet(df, col=\"class\")\n",
    "## Transform (preprocess)\n",
    "transform = Compose(\n",
    "    [\n",
    "        VT.ResizeVideo((224, 224)),\n",
    "        VT.CollectFrames(),\n",
    "        VT.PILVideoToTensor(rescale=True, ordering=\"CTHW\"),\n",
    "    ]\n",
    ")\n",
    "## Sampler (extract frames, make video to images)\n",
    "sampler = samplers.ClipSampler(clip_length=args.clip_length, frame_step=args.frame_step)\n",
    "## Make dataset to enter dataloader of pytorch\n",
    "dataset = datasets.VideoFolderDataset(\n",
    "    root_path=args.dataset_path, label_set=label, transform=transform, sampler=sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "804afa99-643b-4483-b8c3-0d0f488805b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 2666, Validation Size: 667\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dataset)*0.8)\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print(\"Train Size: {}, Validation Size: {}\".format(train_size, val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3931296-5150-4b9e-91e4-8e03972f0b8b",
   "metadata": {},
   "source": [
    "### AutoAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe1ff70-2b9b-432e-85b0-6abdeafc4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, xy):\n",
    "        length = len(xy)\n",
    "        x_temp = [0] * length\n",
    "        y_temp = [0] * length\n",
    "        for i in range(length):\n",
    "            x_temp[i] = xy[i][0]\n",
    "            y_temp[i] = xy[i][1]\n",
    "\n",
    "        self.x = x_temp\n",
    "        self.y = y_temp\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def autoaugment(train_ds, pol=None, device='cpu'):\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    ########################## AUTOAUGMENTATION ##########################\n",
    "    if pol == 'cifar':\n",
    "        policy = transforms.AutoAugmentPolicy.CIFAR10\n",
    "    elif pol == 'imagenet':\n",
    "        policy = transforms.AutoAugmentPolicy.IMAGENET\n",
    "    elif pol == 'svhn':\n",
    "        policy = transforms.AutoAugmentPolicy.SVHN\n",
    "        \n",
    "    if pol == 'cifar' or pol == 'imagenet' or pol == 'svhn':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ConvertImageDtype(torch.uint8),\n",
    "            transforms.AutoAugment(policy),\n",
    "        ])\n",
    "        transform2 = transforms.Compose([\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ])\n",
    "    ######################################################################\n",
    "    if pol == 'cifar' or pol == 'imagenet' or pol == 'svhn':\n",
    "        num_data = len(train_ds)\n",
    "\n",
    "        train_y = torch.stack([torch.from_numpy(np.array(train_ds[i][1])) for i in range(num_data)])\n",
    "        # train_y = train_y.view(num_data, -1)\n",
    "        train_y = torch.tensor(train_y, device=device)\n",
    "\n",
    "        train_x = torch.stack([torch.from_numpy(np.array(train_ds[i][0])) for i in range(num_data)])\n",
    "        train_x = torch.tensor(train_x, device=device)\n",
    "        # train_x = train_x.type(torch.uint8)\n",
    "        transformed_img = []\n",
    "        for j in range(train_x.size(0)):\n",
    "            img = train_x[j, :, :, :, :]\n",
    "            temp = []\n",
    "            for i in range(img.size(1)):\n",
    "                t_img = transform(img[:, i, :, :])\n",
    "                t_img = transform2(t_img)\n",
    "                temp.append(t_img)\n",
    "            transformed_img.append(torch.stack(temp, dim=1))\n",
    "        train_x = torch.stack(transformed_img, dim=0)\n",
    "        # train_x = train_x.type(torch.float32)\n",
    "        train_x = train_x.view(\n",
    "            -1, 3, args.clip_length, train_ds[0][0].shape[2], train_ds[0][0].shape[3]\n",
    "        )\n",
    "\n",
    "        xy = [0] * train_x.shape[0]\n",
    "        for i in range(train_x.shape[0]):\n",
    "            xy[i] = (train_x[i], train_y[i])\n",
    "        train_ds = custom_dataset(xy)\n",
    "\n",
    "        del xy\n",
    "        del train_x\n",
    "        del train_y\n",
    "        gc.collect()\n",
    "\n",
    "        return train_ds\n",
    "    else:\n",
    "        num_data = len(train_ds)\n",
    "\n",
    "        train_y = torch.stack([torch.from_numpy(np.array(train_ds[i][1])) for i in range(num_data)])\n",
    "        train_y = torch.tensor(train_y, device=device)\n",
    "\n",
    "        train_x = torch.stack([torch.from_numpy(np.array(train_ds[i][0])) for i in range(num_data)])\n",
    "        train_x = torch.tensor(train_x, device=device)\n",
    "\n",
    "        xy = [0] * train_x.shape[0]\n",
    "        for i in range(train_x.shape[0]):\n",
    "            xy[i] = (train_x[i], train_y[i])\n",
    "        train_ds = custom_dataset(xy)\n",
    "\n",
    "        del xy\n",
    "        del train_x\n",
    "        del train_y\n",
    "        gc.collect()\n",
    "\n",
    "        return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c9719ea-3937-4e2f-a75b-092d7755bba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data AutoAugmentation is succeed\n",
      "Total Train set samples: 7998, Val set samples: 667\n"
     ]
    }
   ],
   "source": [
    "if args.autoaugment:\n",
    "    ds_original = autoaugment(train_dataset, pol=None, device='cpu')\n",
    "    ds_imnet = autoaugment(train_dataset, pol='imagenet', device='cpu')\n",
    "    ds_svhn = autoaugment(train_dataset, pol='svhn', device='cpu')\n",
    "    # ds_cifar = autoaugment(train_dataset, pol='cifar', device='cpu')\n",
    "                                    \n",
    "    Dset = [ds_original, ds_imnet, ds_svhn]#, ds_svhn, ds_cifar] \n",
    "    \n",
    "    train_dataset = torch.utils.data.ConcatDataset(Dset)\n",
    "    \n",
    "    del ds_original\n",
    "    del ds_imnet\n",
    "    del ds_svhn\n",
    "    # del ds_cifar \n",
    "    del Dset\n",
    "    print(\"Data AutoAugmentation is succeed\")\n",
    "\n",
    "print(\n",
    "    \"Total Train set samples: {}, Val set samples: {}\".format(\n",
    "        len(train_dataset), len(val_dataset)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b3aee-4e3e-46c1-a08b-15633eaed07c",
   "metadata": {},
   "source": [
    "### Dataset Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9416e2d-0b3a-428b-a1f4-af8018f36b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(train_ds, val_ds):\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "    )\n",
    "\n",
    "    trainval_loaders = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "    trainval_sizes = {x: len(trainval_loaders[x].dataset) for x in [\"train\", \"val\"]}\n",
    "\n",
    "    return trainval_loaders, trainval_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016869d1-f0f6-4aea-80db-b86882c2e64c",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a4b4b-34c6-4077-9db3-2b80dafdf799",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "486f3329-78b5-48d5-abe1-5755957a0341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n",
      "Save Name:  TimeSformer-EXP1\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available else revert to CPU\n",
    "device = torch.device(args.device)\n",
    "print(\"Device being used:\", device)\n",
    "\n",
    "saveName = args.model + \"-\" + args.experiment\n",
    "print(\"Save Name: \", saveName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aea47b-1869-427b-93bc-4a0bcc1c5400",
   "metadata": {},
   "source": [
    "### Choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64c7adf1-d69f-4401-94a0-db4528145dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.model == \"TimeSformer\":\n",
    "    model = TimeSformer(\n",
    "        img_size=args.img_size,\n",
    "        num_classes=args.num_classes,\n",
    "        num_frames=args.clip_length,\n",
    "        attention_type=args.attention_type,\n",
    "        pretrained_model=args.pretrained_model,\n",
    "    )\n",
    "elif args.model == \"Efficientnet_LSTM\":\n",
    "    model = Efficientnet_LSTM.net(pretrain_model=args.pretrained_model, embed_size=1280, LSTM_UNITS=64, DO=0.3)\n",
    "elif args.model == \"C3D\":\n",
    "    model = C3D_model.C3D(\n",
    "        model_dir=args.pretrained_model, num_classes=args.num_classes, pretrained=True\n",
    "    )\n",
    "elif args.model == \"R2Plus1D\":\n",
    "    model = R2Plus1D_model.R2Plus1DClassifier(\n",
    "        num_classes=args.num_classes, layer_sizes=(2, 2, 2, 2)\n",
    "    )\n",
    "elif args.model == \"R3D\":\n",
    "    model = R3D_model.R3DClassifier(num_classes=args.num_classes, layer_sizes=(2, 2, 2, 2))\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01690252-d847-43ef-9fb0-1105e8877ec7",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d321bc1-b7dc-4d46-a869-87b06f0dba4e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 121.26M\n",
      "Architecture of TimeSformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): TimeSformer(\n",
       "    (model): VisionTransformer(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (time_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Linear(in_features=768, out_features=9, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total params: %.2fM\" % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "if args.multi_gpu:\n",
    "    model = nn.DataParallel(model)\n",
    "print(\"Architecture of {}\".format(args.model))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efee0b0-720c-4820-864d-b4aa79180f58",
   "metadata": {},
   "source": [
    "### Freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7344c929-74ea-4b35-b92b-60367d17d5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(model.children()):\n",
    "    if i == 0:\n",
    "        for k, param in enumerate(c.parameters()):\n",
    "            if k <= 200:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        print(k)\n",
    "    # print('#'*25)\n",
    "    # print(i)\n",
    "    # print(list(c.parameters()))\n",
    "    # print('#'*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13fb63-f89e-417e-a30e-d4fc6b949096",
   "metadata": {},
   "source": [
    "### Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a0971de-1952-4b47-9532-2116a2b7f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id:  9\n",
      "save directory:  /home/stephencha/Hub/ai-challenge/run/run_009/models\n"
     ]
    }
   ],
   "source": [
    "# build run dir\n",
    "runs = sorted(glob.glob(os.path.join(args.project_dir, \"run\", \"run_*\")))\n",
    "runs.sort()\n",
    "\n",
    "def get_dir_size(path='.'):\n",
    "    total = 0\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "            elif entry.is_dir():\n",
    "                total += get_dir_size(entry.path)\n",
    "    return total\n",
    "\n",
    "size = get_dir_size(runs[-1])\n",
    "\n",
    "if int(size) > 5000: # 디렉토리 용량이 있으면\n",
    "    run_id = int(runs[-1].split(\"_\")[-1]) + 1 if runs else 0 # 다음 번째로 저장\n",
    "else: # 디렉토리 용량이 없으면 거기다 저장\n",
    "    run_id = int(runs[-1].split(\"_\")[-1])\n",
    "print(\"run id: \", run_id)\n",
    "SAVE_DIR = os.path.join(args.project_dir, \"run\", \"run_\" + str(run_id).zfill(3))\n",
    "model_save_dir = os.path.join(SAVE_DIR, \"models\")\n",
    "\n",
    "if int(size) > 5000: # 새로운 디렉토리를 만들어야 할때만 make model directory\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "print(\"save directory: \", model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed117b-9b5f-442e-9b03-5d42ff877a36",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900e492-c23e-4a87-b64e-9858bfdecbd6",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be10d42a-5d9f-4538-831a-c636e392016c",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9e1348c-e3b3-4fe2-b01a-9eeeef61ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model, opt):\n",
    "    if args.model == \"C3D\":\n",
    "        param = [\n",
    "            {\"params\": C3D_model.get_1x_lr_params(model), \"lr\": args.learning_rate},\n",
    "            {\"params\": C3D_model.get_10x_lr_params(model), \"lr\": args.learning_rate * 10},\n",
    "        ]\n",
    "    elif args.model == \"R2Plus1D\":\n",
    "        param = [\n",
    "            {\"params\": R2Plus1D_model.get_1x_lr_params(model), \"lr\": args.learning_rate},\n",
    "            {\"params\": R2Plus1D_model.get_10x_lr_params(model), \"lr\": args.learning_rate * 10},\n",
    "        ]\n",
    "    elif args.model == \"R3D\":\n",
    "        param = model.parameters()\n",
    "    elif args.model == \"TimeSformer\":\n",
    "        param = model.parameters()\n",
    "    elif args.model == \"Efficientnet_LSTM\":\n",
    "        param = model.parameters()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if opt == \"sgd\":\n",
    "        optimizer = optim.SGD(param, lr=args.learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "    elif opt == \"adam\":\n",
    "        optimizer = optim.Adam(param, lr=args.learning_rate, amsgrad=True)\n",
    "    elif opt == \"adamw\":\n",
    "        optimizer = optim.AdamW(param, lr=args.learning_rate)\n",
    "    elif opt == \"adadelta\":\n",
    "        optimizer = optim.Adadelta(param, lr=args.learning_rate)\n",
    "    elif opt == \"nadam\":\n",
    "        optimizer = nadam.Nadam(param, lr=args.learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ed1f2-d13e-4ca7-af88-12944db43428",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1971ab51-1de8-4085-91e7-34d8592190e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss_function(lf):\n",
    "    if lf == \"focal\":\n",
    "        lf = FocalLoss()\n",
    "    elif lf == \"cross_entropy\":\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "    elif lf == \"label_smooth\":\n",
    "        lf = LabelSmoothSoftmaxCEV2()\n",
    "    return lf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe8a37-3df8-4ecf-a025-825eedb95e5d",
   "metadata": {},
   "source": [
    "### Schedular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f47e5671-ba23-406b-9d93-d1abcdc9bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_schedular(optimizer, sche, epochs, length):\n",
    "    if sche == \"step\":\n",
    "        schedular = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif sche == \"onecycle\":\n",
    "        schedular = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            pct_start=0.1,\n",
    "            div_factor=1e5,\n",
    "            max_lr=0.0001,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=length,\n",
    "        )\n",
    "    elif sche == \"cosineannealingwarmrestarts\":\n",
    "        schedular = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=2, eta_min=1e-5, last_epoch=-1\n",
    "        )\n",
    "    elif sche == \"swa\":\n",
    "        schedular = SWALR(optimizer, swa_lr=0.01)\n",
    "    return schedular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28744807-d19e-4a4f-8884-609058dabef2",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab98b66-a57d-476d-aa1b-2a927ad35584",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ac0f898-9f5a-44d6-93b2-28a44451d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    trainval_loaders, trainval_sizes = build_dataset(train_dataset, val_dataset)\n",
    "    \n",
    "    # standard crossentropy loss for classification\n",
    "    criterion = build_loss_function(args.loss_function)\n",
    "    optimizer = build_optimizer(model, opt=args.optimizer)\n",
    "    # the scheduler divides the lr by 10 every 10 epochs\n",
    "    if args.schedular == \"swa\":\n",
    "        optimizer = torchcontrib.optim.SWA(optimizer)\n",
    "    scheduler = build_schedular(\n",
    "        optimizer,\n",
    "        sche=args.schedular,\n",
    "        epochs=args.epoches,\n",
    "        length=trainval_sizes[\"train\"],\n",
    "    )\n",
    "    \n",
    "    best_score = 0  # np.Inf\n",
    "    for epoch in range(args.epoches):\n",
    "        # each epoch has a training and validation step\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            # reset the running loss and corrects\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # set model to train() or eval() mode depending on whether it is trained\n",
    "            # or being validated. Primarily affects layers such as BatchNorm or Dropout.\n",
    "            if phase == \"train\":\n",
    "                # scheduler.step() is to be called once every epoch during training\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_labels, epoch_preds = [], []\n",
    "\n",
    "            for inputs, labels in tqdm(trainval_loaders[phase]):\n",
    "                # move inputs and labels to the device the training is taking place on\n",
    "                inputs = Variable(inputs, requires_grad=True).to(device)\n",
    "                labels = Variable(labels).to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    outputs = model(inputs)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "\n",
    "                probs = nn.Softmax(dim=1)(outputs)\n",
    "                preds = torch.max(probs, 1)[1]\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                epoch_labels.extend(labels.tolist())\n",
    "                epoch_preds.extend(preds.tolist())\n",
    "\n",
    "            epoch_loss = running_loss / trainval_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / trainval_sizes[phase]\n",
    "\n",
    "            epoch_score = f1_score(epoch_preds, epoch_labels, average=\"weighted\")\n",
    "            print(f\"{phase} | EPOCH {epoch} Weighted F1 SCORE: {epoch_score}\")\n",
    "\n",
    "            print(\n",
    "                \"[{}] Epoch: {}/{} Loss: {} Acc: {}\".format(\n",
    "                    phase, epoch + 1, args.epoches, epoch_loss, epoch_acc\n",
    "                )\n",
    "            )\n",
    "            stop_time = timeit.default_timer()\n",
    "            print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
    "\n",
    "            if epoch_score > best_score and phase == \"val\":\n",
    "                print(\n",
    "                    f\"Validation Weighted F1 Score increased ({best_score:.6f} --> {epoch_score:.6f}).  Saving model ...\"\n",
    "                )\n",
    "                model_path = os.path.join(\n",
    "                    model_save_dir,\n",
    "                    saveName\n",
    "                    + \"_epoch-\"\n",
    "                    + str(epoch).zfill(3)\n",
    "                    + \"_epoch_score-{:.6f}.pt\".format(epoch_score)\n",
    "                    + \".pth.tar\",\n",
    "                )\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                        \"opt_dict\": optimizer.state_dict(),\n",
    "                    },\n",
    "                    model_path,\n",
    "                )\n",
    "                print(\"Save model at {}\\n\".format(model_path))\n",
    "                best_score = epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cf39e84-45d7-4331-85b7-53fa69216741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:55<00:00,  1.60s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 0 Weighted F1 SCORE: 0.6018547296102512\n",
      "[train] Epoch: 1/50 Loss: 1.3705883815783981 Acc: 0.5525131282820706\n",
      "Execution time: 355.9610947270121\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 0 Weighted F1 SCORE: 0.7139909989683182\n",
      "[val] Epoch: 1/50 Loss: 0.9596051716733015 Acc: 0.6521739130434783\n",
      "Execution time: 15.573697789019207\n",
      "\n",
      "Validation Weighted F1 Score increased (0.000000 --> 0.713991).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at /home/stephencha/Hub/ai-challenge/run/run_009/models/TimeSformer-EXP1_epoch-000_epoch_score-0.713991.pt.pth.tar\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:49<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 1 Weighted F1 SCORE: 0.6626281825217087\n",
      "[train] Epoch: 2/50 Loss: 1.1160860534726635 Acc: 0.6176544136034009\n",
      "Execution time: 349.671358098014\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 1 Weighted F1 SCORE: 0.7159840393438497\n",
      "[val] Epoch: 2/50 Loss: 1.0268827022641138 Acc: 0.6551724137931034\n",
      "Execution time: 15.529179075994762\n",
      "\n",
      "Validation Weighted F1 Score increased (0.713991 --> 0.715984).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at /home/stephencha/Hub/ai-challenge/run/run_009/models/TimeSformer-EXP1_epoch-001_epoch_score-0.715984.pt.pth.tar\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:49<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 2 Weighted F1 SCORE: 0.6966098758870402\n",
      "[train] Epoch: 3/50 Loss: 0.9651458154293441 Acc: 0.6610402600650164\n",
      "Execution time: 349.6045482440095\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.23it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 2 Weighted F1 SCORE: 0.7077741013496942\n",
      "[val] Epoch: 3/50 Loss: 0.9057325715425312 Acc: 0.6761619190404797\n",
      "Execution time: 15.475953415996628\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:49<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 3 Weighted F1 SCORE: 0.7162742921597881\n",
      "[train] Epoch: 4/50 Loss: 0.9032110760557142 Acc: 0.6809202300575145\n",
      "Execution time: 349.8966647530033\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 3 Weighted F1 SCORE: 0.7656111202956447\n",
      "[val] Epoch: 4/50 Loss: 0.91785152303404 Acc: 0.6821589205397302\n",
      "Execution time: 15.56594216497615\n",
      "\n",
      "Validation Weighted F1 Score increased (0.715984 --> 0.765611).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at /home/stephencha/Hub/ai-challenge/run/run_009/models/TimeSformer-EXP1_epoch-003_epoch_score-0.765611.pt.pth.tar\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:53<00:00,  1.58s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 4 Weighted F1 SCORE: 0.7250656439678591\n",
      "[train] Epoch: 5/50 Loss: 0.8513178900499051 Acc: 0.6921730432608153\n",
      "Execution time: 353.2113901269913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.22it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 4 Weighted F1 SCORE: 0.7603574845235803\n",
      "[val] Epoch: 5/50 Loss: 0.7875463547020302 Acc: 0.7286356821589205\n",
      "Execution time: 15.635076139995363\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:49<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 5 Weighted F1 SCORE: 0.7318670736100041\n",
      "[train] Epoch: 6/50 Loss: 0.8226338528817699 Acc: 0.7039259814953739\n",
      "Execution time: 349.8305208660022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.21it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 5 Weighted F1 SCORE: 0.7482583981848031\n",
      "[val] Epoch: 6/50 Loss: 0.8497547279412242 Acc: 0.7076461769115442\n",
      "Execution time: 15.689555834018392\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:50<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 6 Weighted F1 SCORE: 0.742415781869102\n",
      "[train] Epoch: 7/50 Loss: 0.789042778195784 Acc: 0.7151787946986747\n",
      "Execution time: 350.6515392890142\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.22it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 6 Weighted F1 SCORE: 0.7585621134783762\n",
      "[val] Epoch: 7/50 Loss: 0.7925269771730346 Acc: 0.7226386806596702\n",
      "Execution time: 15.590497897996102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:53<00:00,  1.58s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 7 Weighted F1 SCORE: 0.743174905716243\n",
      "[train] Epoch: 8/50 Loss: 0.7803976726281581 Acc: 0.7164291072768193\n",
      "Execution time: 353.12610643700464\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.21it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 7 Weighted F1 SCORE: 0.75739703596239\n",
      "[val] Epoch: 8/50 Loss: 0.7604305889831668 Acc: 0.7316341829085456\n",
      "Execution time: 15.76454403999378\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:50<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 8 Weighted F1 SCORE: 0.7483753065802463\n",
      "[train] Epoch: 9/50 Loss: 0.7650106425850532 Acc: 0.7228057014253564\n",
      "Execution time: 350.58801147300983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.21it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 8 Weighted F1 SCORE: 0.7635418590907951\n",
      "[val] Epoch: 9/50 Loss: 0.7655180242286808 Acc: 0.7331334332833583\n",
      "Execution time: 15.711730277020251\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:49<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 9 Weighted F1 SCORE: 0.7151994929409557\n",
      "[train] Epoch: 10/50 Loss: 0.8724598752003546 Acc: 0.6864216054013504\n",
      "Execution time: 349.8652889459918\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 9 Weighted F1 SCORE: 0.7774766227871673\n",
      "[val] Epoch: 10/50 Loss: 0.809753553888787 Acc: 0.7376311844077961\n",
      "Execution time: 15.628184342000168\n",
      "\n",
      "Validation Weighted F1 Score increased (0.765611 --> 0.777477).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at /home/stephencha/Hub/ai-challenge/run/run_009/models/TimeSformer-EXP1_epoch-009_epoch_score-0.777477.pt.pth.tar\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:51<00:00,  1.58s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 10 Weighted F1 SCORE: 0.7296869318201938\n",
      "[train] Epoch: 11/50 Loss: 0.8503124069112276 Acc: 0.7041760440110029\n",
      "Execution time: 351.8805051019881\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.22it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 10 Weighted F1 SCORE: 0.7743532014817524\n",
      "[val] Epoch: 11/50 Loss: 0.8621551604463957 Acc: 0.7211394302848575\n",
      "Execution time: 15.580030977987917\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:51<00:00,  1.58s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 11 Weighted F1 SCORE: 0.7332932964124328\n",
      "[train] Epoch: 12/50 Loss: 0.8288924569754518 Acc: 0.7065516379094774\n",
      "Execution time: 351.80180761398515\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 11 Weighted F1 SCORE: 0.7801936667491804\n",
      "[val] Epoch: 12/50 Loss: 0.7890854595840603 Acc: 0.7271364317841079\n",
      "Execution time: 15.80459116998827\n",
      "\n",
      "Validation Weighted F1 Score increased (0.777477 --> 0.780194).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at /home/stephencha/Hub/ai-challenge/run/run_009/models/TimeSformer-EXP1_epoch-011_epoch_score-0.780194.pt.pth.tar\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:50<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 12 Weighted F1 SCORE: 0.7318439899264351\n",
      "[train] Epoch: 13/50 Loss: 0.8239303581802271 Acc: 0.707801950487622\n",
      "Execution time: 350.10454993799794\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 12 Weighted F1 SCORE: 0.7881037399459081\n",
      "[val] Epoch: 13/50 Loss: 0.7519277248425462 Acc: 0.7481259370314842\n",
      "Execution time: 15.829009304987267\n",
      "\n",
      "Validation Weighted F1 Score increased (0.780194 --> 0.788104).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at /home/stephencha/Hub/ai-challenge/run/run_009/models/TimeSformer-EXP1_epoch-012_epoch_score-0.788104.pt.pth.tar\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:50<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 13 Weighted F1 SCORE: 0.7464488705801624\n",
      "[train] Epoch: 14/50 Loss: 0.7929484921116983 Acc: 0.7215553888472119\n",
      "Execution time: 350.1845352429955\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.20it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 13 Weighted F1 SCORE: 0.755395754519485\n",
      "[val] Epoch: 14/50 Loss: 0.752218837278715 Acc: 0.7361319340329835\n",
      "Execution time: 15.813518612005282\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:50<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 14 Weighted F1 SCORE: 0.7503518014117271\n",
      "[train] Epoch: 15/50 Loss: 0.7686762391224179 Acc: 0.7275568892223057\n",
      "Execution time: 350.72902494401205\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.21it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 14 Weighted F1 SCORE: 0.7645061197792312\n",
      "[val] Epoch: 15/50 Loss: 0.823830741188158 Acc: 0.7316341829085456\n",
      "Execution time: 15.6831198099826\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:50<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 15 Weighted F1 SCORE: 0.7466122384645633\n",
      "[train] Epoch: 16/50 Loss: 0.7635034202903352 Acc: 0.7263065766441611\n",
      "Execution time: 350.26571733900346\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.21it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 15 Weighted F1 SCORE: 0.7753254715602161\n",
      "[val] Epoch: 16/50 Loss: 0.7249008132063824 Acc: 0.7451274362818591\n",
      "Execution time: 15.693731268984266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:49<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 16 Weighted F1 SCORE: 0.7519010695929849\n",
      "[train] Epoch: 17/50 Loss: 0.7527769133504494 Acc: 0.730682670667667\n",
      "Execution time: 349.9487448790169\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.21it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 16 Weighted F1 SCORE: 0.7536945168377045\n",
      "[val] Epoch: 17/50 Loss: 0.8354306674968713 Acc: 0.7181409295352323\n",
      "Execution time: 15.673869060992729\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:52<00:00,  1.58s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 17 Weighted F1 SCORE: 0.7557918919331943\n",
      "[train] Epoch: 18/50 Loss: 0.7390643744565273 Acc: 0.7349337334333584\n",
      "Execution time: 352.0982445430127\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.19it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 17 Weighted F1 SCORE: 0.7801170917950075\n",
      "[val] Epoch: 18/50 Loss: 0.7287170030009086 Acc: 0.7571214392803598\n",
      "Execution time: 16.001971908990527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [05:49<00:00,  1.57s/it]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | EPOCH 18 Weighted F1 SCORE: 0.7624054436014883\n",
      "[train] Epoch: 19/50 Loss: 0.7229273899305341 Acc: 0.7423105776444112\n",
      "Execution time: 349.4258628390089\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:15<00:00,  1.21it/s]\n",
      "  0%|          | 0/223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val | EPOCH 18 Weighted F1 SCORE: 0.7780738051281576\n",
      "[val] Epoch: 19/50 Loss: 0.7539025615716445 Acc: 0.7376311844077961\n",
      "Execution time: 15.729514396021841\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 189/223 [04:59<00:53,  1.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-576c191e22fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finish the train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Skip the train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-9ba0ae975000>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if args.train:\n",
    "    train()\n",
    "    print(\"Finish the train\")\n",
    "else:\n",
    "    print(\"Skip the train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4062a15-ca2f-4c6c-b9fd-9c64b66603f9",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512390af-87d8-4f96-91dc-2a37fc5b1d27",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003e2fe-4c79-4cba-be09-aab36adb96a4",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "730a52ed-8cc9-49e6-942f-e7adf6baf304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = samplers.ClipSampler(\n",
    "    clip_length=args.clip_length, frame_step=args.frame_step, test=True\n",
    ")\n",
    "transform = Compose(\n",
    "    [\n",
    "        VT.ResizeVideo((224, 224)),\n",
    "        VT.CollectFrames(),\n",
    "        VT.PILVideoToTensor(rescale=True, ordering=\"CTHW\"),\n",
    "    ]\n",
    ")\n",
    "test_dataset = datasets.VideoFolderDataset(\n",
    "    root_path=args.test_dataset_path, transform=transform, sampler=sampler\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=args.batch_size, num_workers=args.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f174401-2cac-4496-8c93-1619d79ca300",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights from: TimeSformer-EXP1_epoch-012_epoch_score-0.788104.pt.pth.tar...\n",
      "Total params: 121.26M\n"
     ]
    }
   ],
   "source": [
    "runs_pt = os.listdir(model_save_dir)\n",
    "runs_pt.sort()\n",
    "model_path = model_save_dir + \"/\" + runs_pt[-1]  # Latest\n",
    "checkpoint = torch.load(model_path)\n",
    "print(f\"Initializing weights from: {model_path.split('/')[-1]}...\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "print(\"Total params: %.2fM\" % (sum(p.numel() for p in model.parameters()) / 1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc8286-32bd-4baf-82de-93370f4bcd44",
   "metadata": {},
   "source": [
    "### Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0859576-b0cf-4e2e-9dea-398014268fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    model.eval()\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    pred_li = []\n",
    "    for inputs in tqdm(test_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        probs = nn.Softmax(dim=1)(outputs)\n",
    "        preds = torch.max(probs, 1)[1]\n",
    "        pred_li.extend(preds.tolist())\n",
    "\n",
    "    stop_time = timeit.default_timer()\n",
    "    print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
    "    return pred_li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b4694-9760-4b7f-aef1-84f38d2fb8ad",
   "metadata": {},
   "source": [
    "cls_li = {\n",
    "    0: \"driveway_walk\",\n",
    "    1: \"fall_down\",\n",
    "    2: \"fighting\",\n",
    "    3: \"jay_walk\",\n",
    "    4: \"normal\",\n",
    "    5: \"putup_umbrella\",\n",
    "    6: \"ride_cycle\",\n",
    "    7: \"ride_kick\",\n",
    "    8: \"ride_moto\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bca456e-46b7-46e6-8b44-08af855e01e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:16<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 16.99904269797844\n",
      "\n",
      "Finish the inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if args.inference:\n",
    "    pred_li = inference()\n",
    "    print(\"Finish the inference\")\n",
    "else:\n",
    "    print(\"Skip the inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626edb8-c4fe-43e3-8e07-d9ca7e01e830",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5608dd4-dbe8-415d-8ac9-56891f011692",
   "metadata": {},
   "outputs": [],
   "source": [
    "submits = sorted(glob.glob(os.path.join(args.submit_path, \"submit_*\")))\n",
    "submits.sort()\n",
    "\n",
    "if len(submits)==0:\n",
    "    os.makedirs(os.path.join(args.submit_path, \"submit_000\"), exist_ok=True)\n",
    "    submits = sorted(glob.glob(os.path.join(args.submit_path, \"submit_*\")))\n",
    "    submits.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd50911c-0e96-4ee0-aa08-abc96633a79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit id:  1\n",
      "save directory:  ./submit/submit_001\n"
     ]
    }
   ],
   "source": [
    "size = get_dir_size(submits[-1])\n",
    "\n",
    "if int(size) > 5000: # 디렉토리 용량이 있으면\n",
    "    submit_id = int(submits[-1].split(\"_\")[-1]) + 1 if runs else 0 # 다음 번째로 저장\n",
    "else: # 디렉토리 용량이 없으면 거기다 저장\n",
    "    submit_id = int(submits[-1].split(\"_\")[-1])\n",
    "\n",
    "print(\"submit id: \", submit_id)\n",
    "SAVE_DIR = os.path.join(args.submit_path, \"submit_\" + str(submit_id).zfill(3))\n",
    "if int(size) > 5000: # 새로운 디렉토리를 만들어야 할때만 make model directory\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(\"save directory: \", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b5e2b5a-cacc-4f28-860a-75ac00d1bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(args.submit_path + \"/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8520fb2-2411-44cb-87bd-3f3c2a851ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[\"class\"] = [cls_li[int(pred)] for pred in pred_li]\n",
    "sample_submission.to_csv(SAVE_DIR + \"/submit_{}.csv\".format(model_path.split('/')[-1]), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33447764-16b7-41e2-b37c-bc29a8758da8",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
