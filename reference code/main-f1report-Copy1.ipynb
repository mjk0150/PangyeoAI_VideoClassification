{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3d6c55-431e-4c5b-bc9b-65422b8acb30",
   "metadata": {},
   "source": [
    "# 판교 AI Challenge\n",
    "> 참치김치찌개팀<br>\n",
    "> 팀장 손찬영, 팀원 김민정 김하림 이두현 차현수\n",
    "* 과제명 : [아동 및 교통약자 보호를 위한 어린이 도로보행 위험행동 분류 과제]\n",
    "* 과제 링크 : https://www.aiconnect.kr/main/competition/privateDetail/200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716901e-fbd4-4137-a9c8-47394572907b",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2b8db-a350-4118-9775-fd1cfb04ad12",
   "metadata": {},
   "source": [
    "## Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b2715d-e7ab-477a-952a-c00204f3f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "import warnings\n",
    "\n",
    "import easydict\n",
    "import pandas as pd\n",
    "import torchvideo.datasets as datasets\n",
    "import torchvideo.samplers as samplers\n",
    "import torchvideo.transforms as VT\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm.optim.nadam as nadam\n",
    "import torch\n",
    "import torchcontrib\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "from source.focalloss import FocalLoss\n",
    "from source.label_smooth import LabelSmoothSoftmaxCEV2\n",
    "from source.model import C3D_model, R2Plus1D_model, R3D_model\n",
    "from source.model.utils.vit import TimeSformer\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.swa_utils import SWALR\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from torchcontrib.optim import SWA\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f275c544-b678-435e-a06e-c477cd466032",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205f173-f1d1-4f42-bd81-3009d7905e9b",
   "metadata": {},
   "source": [
    "## Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41bc00ea-9722-4ae8-b952-0ec781a2f362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {\n",
    "        ############## Experiment ##############\n",
    "        \"experiment\": \"EXP1\",  # 매번 바꿔준다.\n",
    "        \"project_dir\": os.getcwd(),  # '/home/stephencha/Hub/ai-challenge'\n",
    "        \"train\": False,\n",
    "        \"inference\": True,\n",
    "        \"submit_path\": \"./submit\",\n",
    "        ############## Dataset ##############\n",
    "        \"dataset_path\": \"./dataset/train\",\n",
    "        \"test_dataset_path\": \"./dataset/test\",\n",
    "        \"label_path\": \"./dataset/train_data.csv\",\n",
    "        \"clip_length\": 5,  # slice\n",
    "        \"frame_step\": 1,\n",
    "        \"num_workers\": 8,\n",
    "        \"autoaugment\": True,\n",
    "        \"num_classes\": 9,\n",
    "        ############## Model ##############\n",
    "        \"model\": \"TimeSformer\",  # Options: C3D, R2Plus1D, R3D, TimeSformer, Efficientnet_LSTM\n",
    "        \"attention_type\": \"divided_space_time\",\n",
    "        \"img_size\": 224,\n",
    "        \"pretrained_model\": \"./pretrained/TimeSformer_divST_96x4_224_K600.pyth\",  # ./pretrained/c3d-pretrained.pth, ./pretrained/TimeSformer_divST_96x4_224_K600.pyth, 'efficientnet_b4'\n",
    "        ############## Fine-Tuning ##############\n",
    "        \"randomseed\": False,\n",
    "        \"epoches\": 50,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"loss_function\": \"cross_entropy\",\n",
    "        \"schedular\": \"cosineannealingwarmrestarts\", # cosineannealingwarmrestarts, step\n",
    "        \"batch_size\": 36,  # Depends on VRAM\n",
    "        ############## GPU ##############\n",
    "        \"multi_gpu\": True, \n",
    "        \"device\": \"cuda\",  # \"cpu\" for debugging\n",
    "    }\n",
    ")\n",
    "NAME_ELEMENTS = [args.model, time.strftime(\"%m%d_%H%M\", time.localtime(time.time()))]\n",
    "MODEL_NAME = \"_\".join(NAME_ELEMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647c3f2c-dfea-465c-ae10-291e42d040e0",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2647f-cbf4-486e-a4e8-f2b34fc46bdf",
   "metadata": {},
   "source": [
    "## Randomseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cf009c-314d-440f-b976-d034366f71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.randomseed:\n",
    "    torch.manual_seed(args.randomseed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(args.randomseed)\n",
    "    random.seed(args.randomseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b40161-7dc2-4e73-a587-60e650e93170",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c4669-fb33-4f2d-bfac-cbeee3b3c63f",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d2614ff-50f5-4820-93bf-aa781a8cf2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_li = {\n",
    "    0: \"driveway_walk\", # person\n",
    "    1: \"fall_down\", # person\n",
    "    2: \"fighting\", # person\n",
    "    3: \"jay_walk\", # person\n",
    "    4: \"normal\",\n",
    "    5: \"putup_umbrella\", # person + umbrella\n",
    "    6: \"ride_cycle\", # person + bicycle\n",
    "    7: \"ride_kick\", # person + kickboard\n",
    "    8: \"ride_moto\", # person + motorcycle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0572762-ae40-43ae-9544-dedcc6382007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make string of class to numbers.\n",
    "def string_to_num(row):\n",
    "    if row['class'] == cls_li[0]:\n",
    "        row['class'] = 0\n",
    "    elif row['class'] == cls_li[1]:\n",
    "        row['class'] = 1\n",
    "    elif row['class'] == cls_li[2]:\n",
    "        row['class'] = 2\n",
    "    elif row['class'] == cls_li[3]:\n",
    "        row['class'] = 3\n",
    "    elif row['class'] == cls_li[4]:\n",
    "        row['class'] = 4\n",
    "    elif row['class'] == cls_li[5]:\n",
    "        row['class'] = 5\n",
    "    elif row['class'] == cls_li[6]:\n",
    "        row['class'] = 6\n",
    "    elif row['class'] == cls_li[7]:\n",
    "        row['class'] = 7\n",
    "    elif row['class'] == cls_li[8]:\n",
    "        row['class'] = 8\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af02ef-fda0-4d96-a56e-84c2592b6f10",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63357c2-7974-4c66-885d-fb539bd1cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(args.label_path).set_index('video_filename')\n",
    "df = df.apply(string_to_num, axis='columns')\n",
    "df = df.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff548e12-440e-4c3e-b986-279d61270bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_filename</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>video_0000.mp4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_0001.mp4</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_0002.mp4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_0003.mp4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_0004.mp4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                class\n",
       "video_filename       \n",
       "video_0000.mp4      5\n",
       "video_0001.mp4      7\n",
       "video_0002.mp4      1\n",
       "video_0003.mp4      0\n",
       "video_0004.mp4      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336123c-1ec9-43bc-ae8b-eaa932776433",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694c0e72-2c3c-48d4-936e-710472440d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LABEL\n",
    "label = datasets.CsvLabelSet(df, col=\"class\")\n",
    "## Transform (preprocess)\n",
    "transform = Compose(\n",
    "    [\n",
    "        VT.ResizeVideo((224, 224)),\n",
    "        VT.CollectFrames(),\n",
    "        VT.PILVideoToTensor(rescale=True, ordering=\"CTHW\"),\n",
    "    ]\n",
    ")\n",
    "## Sampler (extract frames, make video to images)\n",
    "sampler = samplers.ClipSampler(clip_length=args.clip_length, frame_step=args.frame_step)\n",
    "## Make dataset to enter dataloader of pytorch\n",
    "dataset = datasets.VideoFolderDataset(\n",
    "    root_path=args.dataset_path, label_set=label, transform=transform, sampler=sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "804afa99-643b-4483-b8c3-0d0f488805b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 2666, Validation Size: 667\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dataset)*0.8)\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print(\"Train Size: {}, Validation Size: {}\".format(train_size, val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3931296-5150-4b9e-91e4-8e03972f0b8b",
   "metadata": {},
   "source": [
    "### AutoAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe1ff70-2b9b-432e-85b0-6abdeafc4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, xy):\n",
    "        length = len(xy)\n",
    "        x_temp = [0] * length\n",
    "        y_temp = [0] * length\n",
    "        for i in range(length):\n",
    "            x_temp[i] = xy[i][0]\n",
    "            y_temp[i] = xy[i][1]\n",
    "\n",
    "        self.x = x_temp\n",
    "        self.y = y_temp\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def autoaugment(train_ds, pol=None, device='cpu'):\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    ########################## AUTOAUGMENTATION ##########################\n",
    "    if pol == 'cifar':\n",
    "        policy = transforms.AutoAugmentPolicy.CIFAR10\n",
    "    elif pol == 'imagenet':\n",
    "        policy = transforms.AutoAugmentPolicy.IMAGENET\n",
    "    elif pol == 'svhn':\n",
    "        policy = transforms.AutoAugmentPolicy.SVHN\n",
    "        \n",
    "    if pol == 'cifar' or pol == 'imagenet' or pol == 'svhn':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ConvertImageDtype(torch.uint8),\n",
    "            transforms.AutoAugment(policy),\n",
    "        ])\n",
    "        transform2 = transforms.Compose([\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ])\n",
    "    ######################################################################\n",
    "    if pol == 'cifar' or pol == 'imagenet' or pol == 'svhn':\n",
    "        num_data = len(train_ds)\n",
    "\n",
    "        train_y = torch.stack([torch.from_numpy(np.array(train_ds[i][1])) for i in range(num_data)])\n",
    "        # train_y = train_y.view(num_data, -1)\n",
    "        train_y = torch.tensor(train_y, device=device)\n",
    "\n",
    "        train_x = torch.stack([torch.from_numpy(np.array(train_ds[i][0])) for i in range(num_data)])\n",
    "        train_x = torch.tensor(train_x, device=device)\n",
    "        # train_x = train_x.type(torch.uint8)\n",
    "        transformed_img = []\n",
    "        for j in range(train_x.size(0)):\n",
    "            img = train_x[j, :, :, :, :]\n",
    "            temp = []\n",
    "            for i in range(img.size(1)):\n",
    "                t_img = transform(img[:, i, :, :])\n",
    "                t_img = transform2(t_img)\n",
    "                temp.append(t_img)\n",
    "            transformed_img.append(torch.stack(temp, dim=1))\n",
    "        train_x = torch.stack(transformed_img, dim=0)\n",
    "        # train_x = train_x.type(torch.float32)\n",
    "        train_x = train_x.view(\n",
    "            -1, 3, args.clip_length, train_ds[0][0].shape[2], train_ds[0][0].shape[3]\n",
    "        )\n",
    "\n",
    "        xy = [0] * train_x.shape[0]\n",
    "        for i in range(train_x.shape[0]):\n",
    "            xy[i] = (train_x[i], train_y[i])\n",
    "        train_ds = custom_dataset(xy)\n",
    "\n",
    "        del xy\n",
    "        del train_x\n",
    "        del train_y\n",
    "        gc.collect()\n",
    "\n",
    "        return train_ds\n",
    "    else:\n",
    "        num_data = len(train_ds)\n",
    "\n",
    "        train_y = torch.stack([torch.from_numpy(np.array(train_ds[i][1])) for i in range(num_data)])\n",
    "        train_y = torch.tensor(train_y, device=device)\n",
    "\n",
    "        train_x = torch.stack([torch.from_numpy(np.array(train_ds[i][0])) for i in range(num_data)])\n",
    "        train_x = torch.tensor(train_x, device=device)\n",
    "\n",
    "        xy = [0] * train_x.shape[0]\n",
    "        for i in range(train_x.shape[0]):\n",
    "            xy[i] = (train_x[i], train_y[i])\n",
    "        train_ds = custom_dataset(xy)\n",
    "\n",
    "        del xy\n",
    "        del train_x\n",
    "        del train_y\n",
    "        gc.collect()\n",
    "\n",
    "        return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c9719ea-3937-4e2f-a75b-092d7755bba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data AutoAugmentation is succeed\n",
      "Total Train set samples: 7998, Val set samples: 667\n"
     ]
    }
   ],
   "source": [
    "if args.autoaugment:\n",
    "    ds_original = autoaugment(train_dataset, pol=None, device='cpu')\n",
    "    ds_imnet = autoaugment(train_dataset, pol='imagenet', device='cpu')\n",
    "    ds_svhn = autoaugment(train_dataset, pol='svhn', device='cpu')\n",
    "    # ds_cifar = autoaugment(train_dataset, pol='cifar', device='cpu')\n",
    "                                    \n",
    "    Dset = [ds_original, ds_imnet, ds_svhn]#, ds_svhn, ds_cifar] \n",
    "    \n",
    "    train_dataset = torch.utils.data.ConcatDataset(Dset)\n",
    "    \n",
    "    del ds_original\n",
    "    del ds_imnet\n",
    "    del ds_svhn\n",
    "    # del ds_cifar \n",
    "    del Dset\n",
    "    print(\"Data AutoAugmentation is succeed\")\n",
    "\n",
    "print(\n",
    "    \"Total Train set samples: {}, Val set samples: {}\".format(\n",
    "        len(train_dataset), len(val_dataset)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b3aee-4e3e-46c1-a08b-15633eaed07c",
   "metadata": {},
   "source": [
    "### Dataset Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9416e2d-0b3a-428b-a1f4-af8018f36b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(train_ds, val_ds):\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,\n",
    "        sampler = ImbalancedDatasetSampler(train_ds),\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_ds,\n",
    "        sampler = ImbalancedDatasetSampler(val_ds),\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "    )\n",
    "\n",
    "    trainval_loaders = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "    trainval_sizes = {x: len(trainval_loaders[x].dataset) for x in [\"train\", \"val\"]}\n",
    "\n",
    "    return trainval_loaders, trainval_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016869d1-f0f6-4aea-80db-b86882c2e64c",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a4b4b-34c6-4077-9db3-2b80dafdf799",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "486f3329-78b5-48d5-abe1-5755957a0341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n",
      "Save Name:  TimeSformer-EXP1\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available else revert to CPU\n",
    "device = torch.device(args.device)\n",
    "print(\"Device being used:\", device)\n",
    "\n",
    "saveName = args.model + \"-\" + args.experiment\n",
    "print(\"Save Name: \", saveName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aea47b-1869-427b-93bc-4a0bcc1c5400",
   "metadata": {},
   "source": [
    "### Choose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64c7adf1-d69f-4401-94a0-db4528145dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.model == \"TimeSformer\":\n",
    "    model = TimeSformer(\n",
    "        img_size=args.img_size,\n",
    "        num_classes=args.num_classes,\n",
    "        num_frames=args.clip_length,\n",
    "        attention_type=args.attention_type,\n",
    "        pretrained_model=args.pretrained_model,\n",
    "    )\n",
    "elif args.model == \"Efficientnet_LSTM\":\n",
    "    model = Efficientnet_LSTM.net(pretrain_model=args.pretrained_model, embed_size=1280, LSTM_UNITS=64, DO=0.3)\n",
    "elif args.model == \"C3D\":\n",
    "    model = C3D_model.C3D(\n",
    "        model_dir=args.pretrained_model, num_classes=args.num_classes, pretrained=True\n",
    "    )\n",
    "elif args.model == \"R2Plus1D\":\n",
    "    model = R2Plus1D_model.R2Plus1DClassifier(\n",
    "        num_classes=args.num_classes, layer_sizes=(2, 2, 2, 2)\n",
    "    )\n",
    "elif args.model == \"R3D\":\n",
    "    model = R3D_model.R3DClassifier(num_classes=args.num_classes, layer_sizes=(2, 2, 2, 2))\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01690252-d847-43ef-9fb0-1105e8877ec7",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d321bc1-b7dc-4d46-a869-87b06f0dba4e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 121.26M\n",
      "Architecture of TimeSformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): TimeSformer(\n",
       "    (model): VisionTransformer(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (time_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (temporal_attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (temporal_fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Linear(in_features=768, out_features=9, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total params: %.2fM\" % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "if args.multi_gpu:\n",
    "    model = nn.DataParallel(model)\n",
    "print(\"Architecture of {}\".format(args.model))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efee0b0-720c-4820-864d-b4aa79180f58",
   "metadata": {},
   "source": [
    "### Freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7344c929-74ea-4b35-b92b-60367d17d5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(model.children()):\n",
    "    if i == 0:\n",
    "        for k, param in enumerate(c.parameters()):\n",
    "            if k <= 200:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        print(k)\n",
    "    # print('#'*25)\n",
    "    # print(i)\n",
    "    # print(list(c.parameters()))\n",
    "    # print('#'*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13fb63-f89e-417e-a30e-d4fc6b949096",
   "metadata": {},
   "source": [
    "### Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a0971de-1952-4b47-9532-2116a2b7f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id:  12\n",
      "save directory:  /home/stephencha/Hub/ai-challenge/run/run_012/models\n"
     ]
    }
   ],
   "source": [
    "# build run dir\n",
    "runs = sorted(glob.glob(os.path.join(args.project_dir, \"run\", \"run_*\")))\n",
    "runs.sort()\n",
    "\n",
    "def get_dir_size(path='.'):\n",
    "    total = 0\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "            elif entry.is_dir():\n",
    "                total += get_dir_size(entry.path)\n",
    "    return total\n",
    "\n",
    "size = get_dir_size(runs[-1])\n",
    "\n",
    "if int(size) > 5000: # 디렉토리 용량이 있으면\n",
    "    run_id = int(runs[-1].split(\"_\")[-1]) + 1 if runs else 0 # 다음 번째로 저장\n",
    "else: # 디렉토리 용량이 없으면 거기다 저장\n",
    "    run_id = int(runs[-1].split(\"_\")[-1])\n",
    "print(\"run id: \", run_id)\n",
    "SAVE_DIR = os.path.join(args.project_dir, \"run\", \"run_\" + str(run_id).zfill(3))\n",
    "model_save_dir = os.path.join(SAVE_DIR, \"models\")\n",
    "\n",
    "if int(size) > 5000: # 새로운 디렉토리를 만들어야 할때만 make model directory\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "print(\"save directory: \", model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed117b-9b5f-442e-9b03-5d42ff877a36",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900e492-c23e-4a87-b64e-9858bfdecbd6",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be10d42a-5d9f-4538-831a-c636e392016c",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9e1348c-e3b3-4fe2-b01a-9eeeef61ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model, opt):\n",
    "    if args.model == \"C3D\":\n",
    "        param = [\n",
    "            {\"params\": C3D_model.get_1x_lr_params(model), \"lr\": args.learning_rate},\n",
    "            {\"params\": C3D_model.get_10x_lr_params(model), \"lr\": args.learning_rate * 10},\n",
    "        ]\n",
    "    elif args.model == \"R2Plus1D\":\n",
    "        param = [\n",
    "            {\"params\": R2Plus1D_model.get_1x_lr_params(model), \"lr\": args.learning_rate},\n",
    "            {\"params\": R2Plus1D_model.get_10x_lr_params(model), \"lr\": args.learning_rate * 10},\n",
    "        ]\n",
    "    elif args.model == \"R3D\":\n",
    "        param = model.parameters()\n",
    "    elif args.model == \"TimeSformer\":\n",
    "        param = model.parameters()\n",
    "    elif args.model == \"Efficientnet_LSTM\":\n",
    "        param = model.parameters()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if opt == \"sgd\":\n",
    "        optimizer = optim.SGD(param, lr=args.learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "    elif opt == \"adam\":\n",
    "        optimizer = optim.Adam(param, lr=args.learning_rate, amsgrad=True)\n",
    "    elif opt == \"adamw\":\n",
    "        optimizer = optim.AdamW(param, lr=args.learning_rate)\n",
    "    elif opt == \"adadelta\":\n",
    "        optimizer = optim.Adadelta(param, lr=args.learning_rate)\n",
    "    elif opt == \"nadam\":\n",
    "        optimizer = nadam.Nadam(param, lr=args.learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ed1f2-d13e-4ca7-af88-12944db43428",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1971ab51-1de8-4085-91e7-34d8592190e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss_function(lf):\n",
    "    if lf == \"focal\":\n",
    "        lf = FocalLoss()\n",
    "    elif lf == \"cross_entropy\":\n",
    "        lf = nn.CrossEntropyLoss()\n",
    "    elif lf == \"label_smooth\":\n",
    "        lf = LabelSmoothSoftmaxCEV2()\n",
    "    return lf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe8a37-3df8-4ecf-a025-825eedb95e5d",
   "metadata": {},
   "source": [
    "### Schedular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f47e5671-ba23-406b-9d93-d1abcdc9bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_schedular(optimizer, sche, epochs, length):\n",
    "    if sche == \"step\":\n",
    "        schedular = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif sche == \"onecycle\":\n",
    "        schedular = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            pct_start=0.1,\n",
    "            div_factor=1e5,\n",
    "            max_lr=0.0001,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=length,\n",
    "        )\n",
    "    elif sche == \"cosineannealingwarmrestarts\":\n",
    "        schedular = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=2, eta_min=1e-5, last_epoch=-1\n",
    "        )\n",
    "    elif sche == \"swa\":\n",
    "        schedular = SWALR(optimizer, swa_lr=0.01)\n",
    "    return schedular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28744807-d19e-4a4f-8884-609058dabef2",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab98b66-a57d-476d-aa1b-2a927ad35584",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ac0f898-9f5a-44d6-93b2-28a44451d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    trainval_loaders, trainval_sizes = build_dataset(train_dataset, val_dataset)\n",
    "    \n",
    "    # standard crossentropy loss for classification\n",
    "    criterion = build_loss_function(args.loss_function)\n",
    "    optimizer = build_optimizer(model, opt=args.optimizer)\n",
    "    # the scheduler divides the lr by 10 every 10 epochs\n",
    "    if args.schedular == \"swa\":\n",
    "        optimizer = torchcontrib.optim.SWA(optimizer)\n",
    "    scheduler = build_schedular(\n",
    "        optimizer,\n",
    "        sche=args.schedular,\n",
    "        epochs=args.epoches,\n",
    "        length=trainval_sizes[\"train\"],\n",
    "    )\n",
    "    \n",
    "    best_score = 0  # np.Inf\n",
    "    for epoch in range(args.epoches):\n",
    "        # each epoch has a training and validation step\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            # reset the running loss and corrects\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # set model to train() or eval() mode depending on whether it is trained\n",
    "            # or being validated. Primarily affects layers such as BatchNorm or Dropout.\n",
    "            if phase == \"train\":\n",
    "                # scheduler.step() is to be called once every epoch during training\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_labels, epoch_preds = [], []\n",
    "\n",
    "            for inputs, labels in tqdm(trainval_loaders[phase]):\n",
    "                # move inputs and labels to the device the training is taking place on\n",
    "                inputs = Variable(inputs, requires_grad=True).to(device)\n",
    "                labels = Variable(labels).to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    outputs = model(inputs)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "\n",
    "                probs = nn.Softmax(dim=1)(outputs)\n",
    "                preds = torch.max(probs, 1)[1]\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                epoch_labels.extend(labels.tolist())\n",
    "                epoch_preds.extend(preds.tolist())\n",
    "\n",
    "            epoch_loss = running_loss / trainval_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / trainval_sizes[phase]\n",
    "            \n",
    "            epoch_score = f1_score(epoch_preds, epoch_labels, average=\"weighted\")\n",
    "            print(f\"{phase} | EPOCH {epoch} Weighted F1 SCORE: {epoch_score}\")\n",
    "            precision, recall, fbeta_score, support = precision_recall_fscore_support(epoch_preds, epoch_labels, average = None)\n",
    "            print(f\" {phase} | EPOCH {epoch} Precision: {precision}\\n Recall: {recall}\")\n",
    "            print(f\" F_beta_score: {fbeta_score}\\n Support: {support}\") \n",
    "            # fbeta_score (if beta==1.0 by default) means recall and precision are equally important.\n",
    "            # The support is the number of occurrences of each class in y_true\n",
    "            print(\n",
    "                \"[{}] Epoch: {}/{} Loss: {} Acc: {}\".format(\n",
    "                    phase, epoch + 1, args.epoches, epoch_loss, epoch_acc\n",
    "                )\n",
    "            )\n",
    "            stop_time = timeit.default_timer()\n",
    "            print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
    "\n",
    "            if epoch_score > best_score and phase == \"val\":\n",
    "                print(\n",
    "                    f\"Validation Weighted F1 Score increased ({best_score:.6f} --> {epoch_score:.6f}).  Saving model ...\"\n",
    "                )\n",
    "                model_path = os.path.join(\n",
    "                    model_save_dir,\n",
    "                    saveName\n",
    "                    + \"_epoch-\"\n",
    "                    + str(epoch).zfill(3)\n",
    "                    + \"_epoch_score-{:.6f}.pt\".format(epoch_score)\n",
    "                    + \".pth.tar\",\n",
    "                )\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                        \"opt_dict\": optimizer.state_dict(),\n",
    "                    },\n",
    "                    model_path,\n",
    "                )\n",
    "                print(\"Save model at {}\\n\".format(model_path))\n",
    "                best_score = epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cf39e84-45d7-4331-85b7-53fa69216741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip the train\n"
     ]
    }
   ],
   "source": [
    "if args.train:\n",
    "    train()\n",
    "    print(\"Finish the train\")\n",
    "else:\n",
    "    print(\"Skip the train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4062a15-ca2f-4c6c-b9fd-9c64b66603f9",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512390af-87d8-4f96-91dc-2a37fc5b1d27",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003e2fe-4c79-4cba-be09-aab36adb96a4",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "730a52ed-8cc9-49e6-942f-e7adf6baf304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = samplers.ClipSampler(\n",
    "    clip_length=args.clip_length, frame_step=args.frame_step, test=True\n",
    ")\n",
    "transform = Compose(\n",
    "    [\n",
    "        VT.ResizeVideo((224, 224)),\n",
    "        VT.CollectFrames(),\n",
    "        VT.PILVideoToTensor(rescale=True, ordering=\"CTHW\"),\n",
    "    ]\n",
    ")\n",
    "test_dataset = datasets.VideoFolderDataset(\n",
    "    root_path=args.test_dataset_path, transform=transform, sampler=sampler\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=args.batch_size, num_workers=args.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67e013b2-587a-4c48-8bca-23acd786c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = '/home/stephencha/Hub/ai-challenge/run/run_011/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f174401-2cac-4496-8c93-1619d79ca300",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights from: TimeSformer-EXP1_epoch-020_epoch_score-0.806589.pt.pth.tar...\n",
      "Total params: 121.26M\n"
     ]
    }
   ],
   "source": [
    "runs_pt = os.listdir(model_save_dir)\n",
    "runs_pt.sort()\n",
    "model_path = model_save_dir + \"/\" + runs_pt[-1]  # Latest\n",
    "checkpoint = torch.load(model_path)\n",
    "print(f\"Initializing weights from: {model_path.split('/')[-1]}...\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "print(\"Total params: %.2fM\" % (sum(p.numel() for p in model.parameters()) / 1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc8286-32bd-4baf-82de-93370f4bcd44",
   "metadata": {},
   "source": [
    "### Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0859576-b0cf-4e2e-9dea-398014268fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    model.eval()\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    pred_li = []\n",
    "    for inputs in tqdm(test_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        probs = nn.Softmax(dim=1)(outputs)\n",
    "        preds = torch.max(probs, 1)[1]\n",
    "        pred_li.extend(preds.tolist())\n",
    "\n",
    "    stop_time = timeit.default_timer()\n",
    "    print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
    "    return pred_li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b4694-9760-4b7f-aef1-84f38d2fb8ad",
   "metadata": {},
   "source": [
    "cls_li = {\n",
    "    0: \"driveway_walk\",\n",
    "    1: \"fall_down\",\n",
    "    2: \"fighting\",\n",
    "    3: \"jay_walk\",\n",
    "    4: \"normal\",\n",
    "    5: \"putup_umbrella\",\n",
    "    6: \"ride_cycle\",\n",
    "    7: \"ride_kick\",\n",
    "    8: \"ride_moto\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bca456e-46b7-46e6-8b44-08af855e01e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:20<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 20.902910352975596\n",
      "\n",
      "Finish the inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if args.inference:\n",
    "    pred_li = inference()\n",
    "    print(\"Finish the inference\")\n",
    "else:\n",
    "    print(\"Skip the inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626edb8-c4fe-43e3-8e07-d9ca7e01e830",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5608dd4-dbe8-415d-8ac9-56891f011692",
   "metadata": {},
   "outputs": [],
   "source": [
    "submits = sorted(glob.glob(os.path.join(args.submit_path, \"submit_*\")))\n",
    "submits.sort()\n",
    "\n",
    "if len(submits)==0:\n",
    "    os.makedirs(os.path.join(args.submit_path, \"submit_000\"), exist_ok=True)\n",
    "    submits = sorted(glob.glob(os.path.join(args.submit_path, \"submit_*\")))\n",
    "    submits.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd50911c-0e96-4ee0-aa08-abc96633a79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit id:  3\n",
      "save directory:  ./submit/submit_003\n"
     ]
    }
   ],
   "source": [
    "size = get_dir_size(submits[-1])\n",
    "\n",
    "if int(size) > 5000: # 디렉토리 용량이 있으면\n",
    "    submit_id = int(submits[-1].split(\"_\")[-1]) + 1 if runs else 0 # 다음 번째로 저장\n",
    "else: # 디렉토리 용량이 없으면 거기다 저장\n",
    "    submit_id = int(submits[-1].split(\"_\")[-1])\n",
    "\n",
    "print(\"submit id: \", submit_id)\n",
    "SAVE_DIR = os.path.join(args.submit_path, \"submit_\" + str(submit_id).zfill(3))\n",
    "if int(size) > 5000: # 새로운 디렉토리를 만들어야 할때만 make model directory\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(\"save directory: \", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b5e2b5a-cacc-4f28-860a-75ac00d1bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(args.submit_path + \"/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8520fb2-2411-44cb-87bd-3f3c2a851ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[\"class\"] = [cls_li[int(pred)] for pred in pred_li]\n",
    "sample_submission.to_csv(SAVE_DIR + \"/submit_{}.csv\".format(model_path.split('/')[-1]), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33447764-16b7-41e2-b37c-bc29a8758da8",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
