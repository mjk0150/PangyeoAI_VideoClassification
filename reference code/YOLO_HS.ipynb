{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d0abf4-a9ab-4bd8-9f53-0a19ffb42484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import easydict\n",
    "from source.model.yolo.detect import run as yolo\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35768f7e-3b00-4562-b62f-d454c8238a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {\n",
    "        \"source\": './dataset/train/video_0000.mp4',# Ï†ÑÏ≤¥ ÎèôÏòÅÏÉÅ Í∞ñÍ≥†Ïò§Í∏∞: './dataset/train/*.mp4',\n",
    "        # \"save_crop\": True,\n",
    "        \"imgsz\": [1024, 1024],\n",
    "        \"device\": '0, 1',\n",
    "        \"project\": './dataset/train_yolo',\n",
    "        \"save_txt\": True,\n",
    "        # \"classes\": 0,\n",
    "        \"clip_len\": 5,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d31ae94-2769-4df9-ae00-f5d1228afb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "        'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "        'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "        'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', \n",
    "        'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', \n",
    "        'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30776588-3e74-43ca-a977-b3fbed320b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person umbrella bicycle motorcycle\n"
     ]
    }
   ],
   "source": [
    "print(names[0], names[25], names[1], names[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ffd4f3-715b-45c5-ad7f-6969715df3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3f0a3d4-2aed-43bf-8928-4819a838e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_li = {\n",
    "    0: \"driveway_walk\", # person\n",
    "    1: \"fall_down\", # person\n",
    "    2: \"fighting\", # person\n",
    "    3: \"jay_walk\", # person\n",
    "    4: \"normal\", # nothing..?\n",
    "    5: \"putup_umbrella\", # person + umbrella\n",
    "    6: \"ride_cycle\", # person + bicycle\n",
    "    7: \"ride_kick\", # person\n",
    "    8: \"ride_moto\", # person + motorcycle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c49aa36b-db5e-4de9-9898-c8170f50e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make string of class to numbers.\n",
    "def string_to_num(row):\n",
    "    if row['class'] == cls_li[0]:\n",
    "        row['class'] = 0\n",
    "    elif row['class'] == cls_li[1]:\n",
    "        row['class'] = 1\n",
    "    elif row['class'] == cls_li[2]:\n",
    "        row['class'] = 2\n",
    "    elif row['class'] == cls_li[3]:\n",
    "        row['class'] = 3\n",
    "    elif row['class'] == cls_li[4]:\n",
    "        row['class'] = 4\n",
    "    elif row['class'] == cls_li[5]:\n",
    "        row['class'] = 5\n",
    "    elif row['class'] == cls_li[6]:\n",
    "        row['class'] = 6\n",
    "    elif row['class'] == cls_li[7]:\n",
    "        row['class'] = 7\n",
    "    elif row['class'] == cls_li[8]:\n",
    "        row['class'] = 8\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaacf8b3-b122-4a9d-9e6b-999e58eedf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/train_data.csv\").set_index('video_filename')\n",
    "df = df.apply(string_to_num, axis='columns')\n",
    "df = df.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40f3012b-8d2d-472a-9854-6e158fa886d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_filename</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>video_0000.mp4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_0001.mp4</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_0002.mp4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_0003.mp4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_0004.mp4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                class\n",
       "video_filename       \n",
       "video_0000.mp4      5\n",
       "video_0001.mp4      7\n",
       "video_0002.mp4      1\n",
       "video_0003.mp4      0\n",
       "video_0004.mp4      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d6610bb-124f-471a-8a99-7ee0200baea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 179335f torch 1.8.2 CUDA:0 (NVIDIA GeForce RTX 3090, 24268.3125MB)\n",
      "                              CUDA: 1 (NVIDIA Graphics Device, 16117.3125MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7225885 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video 1/1 (1/75) /home/stephencha/Hub/ai-challenge/dataset/train/video_0000.mp4: 576x1024 2 persons, 4 cars, 1 traffic light, 2 umbrellas, Done. (0.008s)\n",
      "video 1/1 (16/75) /home/stephencha/Hub/ai-challenge/dataset/train/video_0000.mp4: 576x1024 2 persons, 4 cars, 1 traffic light, 2 umbrellas, Done. (0.006s)\n",
      "video 1/1 (31/75) /home/stephencha/Hub/ai-challenge/dataset/train/video_0000.mp4: 576x1024 2 persons, 4 cars, 1 traffic light, 3 umbrellas, Done. (0.006s)\n",
      "video 1/1 (46/75) /home/stephencha/Hub/ai-challenge/dataset/train/video_0000.mp4: 576x1024 3 persons, 5 cars, 2 umbrellas, Done. (0.006s)\n",
      "video 1/1 (61/75) /home/stephencha/Hub/ai-challenge/dataset/train/video_0000.mp4: 576x1024 3 persons, 4 cars, 1 traffic light, 2 umbrellas, Done. (0.006s)\n",
      "Speed: 0.3ms pre-process, 6.0ms inference, 0.7ms NMS per image at shape (1, 3, 1024, 1024)\n",
      "Results saved to \u001b[1mdataset/train_yolo\u001b[0m\n",
      "5 labels saved to dataset/train_yolo/labels\n",
      "[(0, 1, 2, [283.0, 135.0, 303.0, 149.0]), (0, 1, 2, [97.0, 122.0, 114.0, 140.0]), (0, 1, 9, [94.0, 176.0, 111.0, 210.0]), (0, 1, 2, [129.0, 127.0, 152.0, 141.0]), (0, 1, 0, [248.0, 188.0, 264.0, 214.0]), (0, 1, 25, [243.0, 168.0, 272.0, 189.0]), (0, 1, 2, [389.0, 141.0, 426.0, 162.0]), (0, 1, 25, [218.0, 158.0, 248.0, 178.0]), (0, 1, 0, [224.0, 174.0, 239.0, 205.0]), (0, 16, 2, [100.0, 132.0, 114.0, 142.0]), (0, 16, 9, [93.0, 179.0, 110.0, 210.0]), (0, 16, 2, [283.0, 135.0, 301.0, 149.0]), (0, 16, 2, [130.0, 127.0, 151.0, 141.0]), (0, 16, 0, [244.0, 187.0, 259.0, 217.0]), (0, 16, 0, [221.0, 175.0, 235.0, 204.0]), (0, 16, 2, [390.0, 141.0, 426.0, 162.0]), (0, 16, 25, [212.0, 154.0, 242.0, 175.0]), (0, 16, 25, [237.0, 168.0, 269.0, 188.0]), (0, 31, 2, [100.0, 131.0, 114.0, 142.0]), (0, 31, 25, [216.0, 158.0, 254.0, 179.0]), (0, 31, 9, [93.0, 179.0, 110.0, 210.0]), (0, 31, 2, [283.0, 135.0, 301.0, 149.0]), (0, 31, 2, [129.0, 126.0, 152.0, 141.0]), (0, 31, 0, [212.0, 174.0, 230.0, 199.0]), (0, 31, 0, [238.0, 185.0, 254.0, 210.0]), (0, 31, 2, [390.0, 141.0, 425.0, 162.0]), (0, 31, 25, [208.0, 155.0, 239.0, 173.0]), (0, 31, 25, [234.0, 166.0, 261.0, 187.0]), (0, 46, 0, [229.0, 164.0, 255.0, 189.0]), (0, 46, 2, [282.0, 135.0, 301.0, 149.0]), (0, 46, 2, [99.0, 130.0, 114.0, 142.0]), (0, 46, 2, [654.0, 172.0, 768.0, 207.0]), (0, 46, 2, [129.0, 127.0, 152.0, 141.0]), (0, 46, 0, [235.0, 183.0, 248.0, 210.0]), (0, 46, 2, [389.0, 141.0, 424.0, 162.0]), (0, 46, 0, [213.0, 171.0, 226.0, 197.0]), (0, 46, 25, [228.0, 166.0, 257.0, 185.0]), (0, 46, 25, [206.0, 154.0, 235.0, 173.0]), (0, 61, 9, [93.0, 178.0, 111.0, 210.0]), (0, 61, 0, [226.0, 163.0, 250.0, 193.0]), (0, 61, 2, [283.0, 135.0, 302.0, 149.0]), (0, 61, 2, [99.0, 129.0, 114.0, 142.0]), (0, 61, 0, [210.0, 171.0, 224.0, 195.0]), (0, 61, 0, [231.0, 180.0, 244.0, 205.0]), (0, 61, 2, [129.0, 126.0, 152.0, 141.0]), (0, 61, 2, [391.0, 141.0, 425.0, 162.0]), (0, 61, 25, [202.0, 154.0, 230.0, 171.0]), (0, 61, 25, [223.0, 164.0, 251.0, 184.0])]\n"
     ]
    }
   ],
   "source": [
    "path = yolo(\n",
    "    source=args.source,\n",
    "    clip_len=args.clip_len,\n",
    "    # save_crop=args.save_crop,\n",
    "    imgsz=args.imgsz,\n",
    "    device=args.device,\n",
    "    project=args.project,\n",
    "    save_txt=args.save_txt,\n",
    "    # classes=args.classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c0968-c850-4961-a8a6-fac9fd52dae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78257a08-8b1f-4654-8442-5bac06c1e90d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5852e36f7d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"video number: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"frame number: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "filenames = os.listdir(\"/home/stephencha/Hub/ai-challenge/\"+str(path)+\"/labels\")\n",
    "filenames.sort()\n",
    "for f in filenames:\n",
    "    a = f.split(\"_\", 2)\n",
    "    b = a[2].split(\".\")\n",
    "    print(\"video number: \", a[1])\n",
    "    print(\"frame number: \", b[0])\n",
    "    \n",
    "    file = open(f, \"r\")\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        line = line.split()\n",
    "        if not line: break\n",
    "        \n",
    "        cls_obj = int(line[0])\n",
    "        centre_obj_x = float(line[1])\n",
    "        centre_obj_y = float(line[2])\n",
    "        width = float(line[3])\n",
    "        height = float(line[4])\n",
    "        print(\"class: \", names[cls_obj], \", Center Coordinate: \", (centre_obj_x, centre_obj_y), \", Width and Height: \", (width, height))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9bb47-d667-4be8-822a-8674d5c26bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6d27d-d45c-4c79-924e-6a435b8e9a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c0f1d-cd82-44d5-87fb-9d02f6bde7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a3e266-de74-4cc2-b0e1-12fbba5bb45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45948d1-a4a1-4dbb-85e8-67d776670887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b74f4-ec73-4102-a199-b148084d5acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ead76-a35c-4048-9641-c38119bada3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09ba64-2afe-4247-bf05-0d9f666447bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
